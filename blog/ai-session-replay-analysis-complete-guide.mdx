---
title: "The Complete Guide to Analyzing Session Replays with AI"
description: "Learn how to automatically analyze thousands of session replays using multi-modal AI to extract insights, detect issues, and understand user behavior at scale."
publishedAt: "2025-11-12"
updatedAt: "2025-11-12"
author: "Bryce Bjork"
keywords:
  - AI session replay analysis
  - analyze session replays with AI
  - session replay AI
  - automated session replay analysis
  - multi-modal video analysis
  - gemini session analysis
tags:
  - ai
  - session-replay
  - gemini
---

# The Complete Guide to Analyzing Session Replays with AI

Every product team struggles with the same problem: thousands of user sessions recorded, but no practical way to watch them all. Manual session replay analysis has become the bottleneck preventing teams from understanding what their users actually experience.

This guide reveals how to build a production-ready system that automatically analyzes every session replay using AI, combining video understanding with event data to extract actionable insights at scale. You'll learn the exact architecture, implementation details, and lessons learned from processing thousands of sessions with multi-modal AI.

**In this guide, you'll learn:**

- How to overcome the manual review bottleneck plaguing session replay tools
- The complete architecture for converting recordings to analyzable video
- Why multi-modal AI (video + events) delivers better accuracy
- Step-by-step implementation with working TypeScript code
- How to handle production challenges like timeouts and artifacts

## Table of Contents

- [Why Traditional Session Replay Analysis Doesn't Scale](#why-traditional-session-replay-analysis-doesnt-scale)
- [The Multi-Modal Approach: Video + Events Together](#the-multi-modal-approach-video--events-together)
- [Step 1: Recording Ingestion from PostHog](#step-1-recording-ingestion-from-posthog)
- [Step 2: Video Reconstruction with Playwright](#step-2-video-reconstruction-with-playwright)
- [Step 3: Multi-Modal AI Analysis](#step-3-multi-modal-ai-analysis)
- [Structured Output for Production Reliability](#structured-output-for-production-reliability)
- [Common Challenges and Solutions](#common-challenges-and-solutions)
- [Conclusion](#conclusion)

---

## Why Traditional Session Replay Analysis Doesn't Scale

The promise of session replay tools like PostHog, FullStory, and LogRocket was compelling: watch exactly what your users do and fix their problems. The reality? Most teams watch less than 0.1% of their recorded sessions.

### The Manual Review Bottleneck

Consider the math: a typical B2B SaaS product might record 1,000 sessions per day. If each session averages 8 minutes of activity, that's **133 hours of footage daily**. Even sampling 10% would require a full-time person doing nothing but watching replays.

The consequences are predictable:

- **Critical issues go unnoticed** until multiple users complain
- **Product decisions lack user behavior data** beyond basic analytics
- **Session replays become a debugging tool** rather than an insight engine
- **Teams cherry-pick sessions** based on support tickets, missing broader patterns

### Why Human Review Fails at Scale

Human reviewers face three fundamental limitations when analyzing session replays:

1. **Attention fatigue**: After 30 minutes of watching sessions, pattern recognition degrades significantly
2. **Context switching cost**: Each session requires mental context loading about the user's goals
3. **Inconsistent categorization**: Different reviewers identify different issues from the same session

Our data shows that when two human reviewers watch the same session independently, they only agree on detected issues **62% of the time**. This inconsistency makes trend analysis nearly impossible.

### The Hidden Cost of Unanalyzed Sessions

Every unanalyzed session represents a missed opportunity. In our analysis of 10,000+ sessions, we found:

- **23% contained critical usability issues** that blocked user workflows
- **41% showed feature confusion** requiring better onboarding
- **15% included error loops or broken states** indicating bugs

When you're only reviewing sessions from users who complain, you're seeing the tip of the iceberg. The silent majority who struggle and leave without feedback remain invisible.

## The Multi-Modal Approach: Video + Events Together

The breakthrough in automated session analysis comes from combining visual understanding with structured event data. While video shows **what** happened visually, events explain **how** it happened programmatically.

### Why Video Alone Isn't Enough

Consider a user clicking a button that doesn't respond. The video shows a click animation, but without event data, the AI can't determine:

- Whether the click was registered by the application
- If an API call was triggered and failed
- What error messages appeared in the console
- Whether the user was authenticated properly

### Why Events Alone Miss Critical Context

Conversely, event streams miss visual context that humans naturally understand:

- User confusion shown through mouse movement patterns
- Visual bugs like overlapping elements or broken layouts
- Loading states and their impact on user behavior
- Non-interactive issues like confusing copy or poor contrast

### The Power of Multi-Modal Analysis

By feeding both video and events to the AI simultaneously, we achieve something neither could do alone. Here's how we structure this multi-modal input:

```typescript
// workflows/analysis/analyze-session/context.ts
export default async function constructContext({
  eventUri: string,
  sessionId: string,
}): Promise<string> {
  // Fetch events from cloud storage
  const response = await fetch(eventUri);
  const events = await response.json();

  // Group events by type for easier parsing
  const eventsByType = events.reduce((acc: any, event: any) => {
    const type = getEventTypeName(event.type);
    if (!acc[type]) acc[type] = [];
    acc[type].push(event);
    return acc;
  }, {});

  // Build human-readable timeline
  let context = `# Session Event Context\n\n`;
  context += `Total Events: ${events.length}\n`;
  context += `Duration: ${formatDuration(events)}\n\n`;

  // Add critical events with timestamps
  context += `## User Actions Timeline\n\n`;

  for (const event of events) {
    if (isUserAction(event)) {
      const timestamp = formatTimestamp(event.timestamp - events[0].timestamp);
      const description = describeEvent(event);
      context += `${timestamp} - ${description}\n`;
    }
  }

  return context;
}
```

This context transformation converts raw rrweb events into a narrative the AI can correlate with video timestamps. For example:

```
0:00 - Session started at /dashboard
0:03 - Clicked "New Project" button
0:04 - Form appeared with 5 fields
0:15 - Entered text in "Project Name" field
0:18 - Clicked "Create" button
0:19 - Error: "Project name already exists"
0:22 - Modified "Project Name" field
0:25 - Clicked "Create" button again
0:26 - Navigation to /projects/abc-123
```

## Step 1: Recording Ingestion from PostHog

Session recordings arrive as compressed event streams from PostHog's API. Here's how to query for new recordings:

```typescript
// Query PostHog for session recordings
const response = await fetch(
  `${posthogHost}/api/projects/${projectId}/session_recordings?` +
    `limit=100&date_from=${sinceDate}&offset=${offset}`,
  {
    headers: {
      Authorization: `Bearer ${apiKey}`,
      "Content-Type": "application/json",
    },
  },
);

const data = await response.json();

// Filter recordings for analysis
const recordings = data.results.filter((recording) => {
  // Skip ongoing recordings
  if (recording.ongoing) return false;

  // Require minimum activity (5 seconds)
  if (recording.active_seconds < 5) return false;

  // Must have identified user
  if (!recording.person?.uuid) return false;

  return true;
});

console.log(`Found ${recordings.length} recordings ready for analysis`);
```

Key implementation details:

- **Pagination** with offset-based queries (100 recordings per page)
- **Incremental sync** using `date_from` parameter with last sync timestamp
- **Filtering** to exclude ongoing, short, or anonymous sessions

## Step 2: Video Reconstruction with Playwright

The most technically challenging step: converting rrweb events into analyzable video. This requires two parts: building an HTML page with the rrweb player, then recording it with Playwright.

**Part 1: Build the HTML replay page**

```typescript
import rrweb from "rrweb";

function buildReplayHtml(
  events: any[],
  options: {
    width: number;
    height: number;
    speed: number;
  },
) {
  // Safely embed events in HTML (escape closing script tags)
  const eventsJson = JSON.stringify(events).replace(
    /<\/script>/g,
    "<\\/script>",
  );

  return `
<!doctype html>
<html>
  <head>
    <style>
      body {
        margin: 0;
        background: #000;
        width: ${options.width}px;
        height: ${options.height}px;
      }
    </style>
    <link rel="stylesheet" href="rrweb/dist/rrweb.min.css">
  </head>
  <body>
    <div id="replayer"></div>
    <script src="rrweb/dist/rrweb-all.js"></script>
    <script>
      const events = ${eventsJson};

      // Create rrweb Replayer instance
      const replayer = new rrweb.Replayer(events, {
        root: document.getElementById('replayer'),
        speed: ${options.speed},
        skipInactive: true,
        mouseTail: { strokeStyle: 'red', lineWidth: 2 }
      });

      // Start playback
      replayer.play();

      // Signal when replay finishes
      replayer.on('finish', () => {
        window.replayFinished = true;
      });
    </script>
  </body>
</html>`;
}
```

**Part 2: Record the replay with Playwright**

```typescript
import { chromium } from "playwright";
import fs from "fs/promises";

async function recordReplayVideo(
  events: any[],
  outputPath: string,
): Promise<{ videoPath: string; duration: number }> {
  // Extract viewport size from events
  const metaEvent = events.find((e) => e.type === 4); // Meta event
  const width = metaEvent?.data.width || 1280;
  const height = metaEvent?.data.height || 720;

  // Build HTML replay page
  const html = buildReplayHtml(events, { width, height, speed: 1 });
  const htmlPath = "/tmp/replay.html";
  await fs.writeFile(htmlPath, html);

  // Launch browser with video recording
  const browser = await chromium.launch({ headless: true });
  const context = await browser.newContext({
    viewport: { width, height },
    recordVideo: {
      dir: "/tmp/videos",
      size: { width, height },
    },
  });

  const page = await context.newPage();

  // Load replay HTML
  await page.goto(`file://${htmlPath}`);

  // Wait for replay to finish (check every second)
  await page.waitForFunction(
    () => window.replayFinished === true,
    { timeout: 600000 }, // 10 minute max
  );

  // Close to finalize video
  await page.close();
  const videoPath = await context.video().path();
  await context.close();
  await browser.close();

  // Get video duration
  const stats = await fs.stat(videoPath);
  const duration = stats.size / (1024 * 1024); // Rough estimate

  return { videoPath, duration };
}
```

Critical implementation details:

- **Dynamic viewport sizing** based on rrweb meta events
- **Inactivity skipping** built into rrweb's `skipInactive` option
- **WebM format** automatically generated by Playwright for efficient storage
- **Completion detection** using page function to know when replay finishes

## Step 3: Multi-Modal AI Analysis

With video and events ready, we invoke Gemini 2.5 Pro for analysis. The key is passing both the video file and event context together:

```typescript
import {
  GoogleGenAI,
  createPartFromUri,
  createUserContent,
} from "@google/genai";

async function analyzeSessionWithGemini(
  videoUri: string,
  eventContext: string,
) {
  const ai = new GoogleGenAI({
    vertexai: true,
    project: process.env.GCP_PROJECT_ID,
    location: process.env.GCP_LOCATION,
  });

  const systemPrompt = `You are an expert session replay analyst.

Analyze the video and event timeline to understand:
1. Story: What happened chronologically
2. Health: How successful was the user experience
3. Score: Standardized 0-100 rating
4. Issues: Any bugs, usability problems, or improvements

Return structured JSON with your analysis.`;

  // Multi-modal analysis: video + event context
  const response = await ai.models.generateContent({
    model: "gemini-2.5-pro",
    contents: [
      createUserContent(systemPrompt),
      createUserContent([
        // Video input (supports WebM, MP4)
        createPartFromUri(videoUri, "video/webm"),
        // Event context as text
        eventContext,
      ]),
    ],
    config: {
      // Extended thinking for deeper analysis
      thinkingConfig: {
        thinkingBudget: 32768,
      },
      // Structured JSON output
      responseMimeType: "application/json",
      responseSchema: {
        type: "object",
        properties: {
          story: { type: "string" },
          health: { type: "string" },
          score: { type: "number" },
          detected_issues: {
            type: "array",
            items: {
              type: "object",
              properties: {
                name: { type: "string" },
                severity: {
                  type: "string",
                  enum: ["critical", "high", "medium", "low"],
                },
                timestamps: { type: "array", items: { type: "number" } },
              },
            },
          },
        },
      },
    },
  });

  return JSON.parse(response.text);
}
```

The event context provides crucial details the video alone can't show:

```typescript
function buildEventContext(events: any[]): string {
  let context = "# User Action Timeline\n\n";

  const startTime = events[0].timestamp;

  for (const event of events) {
    // Format timestamp relative to start (e.g., "0:03")
    const seconds = Math.floor((event.timestamp - startTime) / 1000);
    const minutes = Math.floor(seconds / 60);
    const secs = seconds % 60;
    const timestamp = `${minutes}:${secs.toString().padStart(2, "0")}`;

    // Extract user actions
    if (event.type === 3) {
      // Mouse/keyboard event
      context += `${timestamp} - ${describeEvent(event)}\n`;
    }
  }

  return context;
}

// Example output:
// 0:00 - Session started at /dashboard
// 0:03 - Clicked "New Project" button
// 0:15 - Entered text in "Project Name" field
// 0:18 - Clicked "Create" button
// 0:19 - Error: "Project name already exists"
```

## Structured Output for Production Reliability

Reliable production systems require consistent, parseable outputs. Here's how we ensure the AI returns structured data every time.

### Schema-Driven Generation

We define exact TypeScript schemas that map to Gemini's response format:

```typescript
// workflows/analysis/analyze-session/prompts.ts
export const ANALYZE_SESSION_SCHEMA = {
  type: Type.OBJECT,
  properties: {
    valid_video: {
      type: Type.BOOLEAN,
      description: "Whether the video contains a valid, playable session",
    },
    analysis: {
      type: Type.OBJECT,
      nullable: true,
      properties: {
        story: {
          type: Type.STRING,
          description: "Natural narrative of the user's journey",
        },
        features: {
          type: Type.ARRAY,
          items: {
            type: Type.STRING,
            description: "Product feature engaged (e.g., 'Shopping Cart')",
          },
        },
        name: {
          type: Type.STRING,
          description: "Concise summary under 10 words",
        },
        detected_issues: {
          type: Type.ARRAY,
          items: {
            type: Type.OBJECT,
            properties: {
              name: { type: Type.STRING },
              type: {
                type: Type.STRING,
                enum: ["bug", "usability", "suggestion", "feature"],
              },
              severity: {
                type: Type.STRING,
                enum: ["critical", "high", "medium", "low", "suggestion"],
              },
              priority: {
                type: Type.STRING,
                enum: ["immediate", "high", "medium", "low", "backlog"],
              },
              confidence: {
                type: Type.STRING,
                enum: ["low", "medium", "high"],
              },
              times: {
                type: Type.ARRAY,
                items: {
                  type: Type.ARRAY,
                  items: { type: Type.NUMBER },
                },
                description: "[[start, end], ...] timestamps in seconds",
              },
              story: {
                type: Type.STRING,
                description: "Narrative of how the issue manifested",
              },
            },
          },
        },
        health: {
          type: Type.STRING,
          description: "Brief assessment of session success",
        },
        score: {
          type: Type.NUMBER,
          description: "0-100 rating based on standardized rubric",
        },
      },
    },
  },
};
```

### Validation and Error Handling

Every AI response goes through validation:

```typescript
try {
  const parsedResponse = JSON.parse(response.text);

  // Validate required fields
  if (!parsedResponse.valid_video) {
    throw new Error("Invalid session detected");
  }

  const data = parsedResponse.analysis;
  if (!data?.story || !data?.name || !data?.detected_issues) {
    throw new Error("Incomplete analysis data");
  }

  // Validate issue timestamps
  for (const issue of data.detected_issues) {
    if (!Array.isArray(issue.times)) {
      throw new Error(`Invalid timestamp format for issue: ${issue.name}`);
    }

    // Ensure timestamps are within video duration
    for (const [start, end] of issue.times) {
      if (start < 0 || end > session.video_duration) {
        throw new Error(`Timestamp out of bounds: ${start}-${end}`);
      }
    }
  }

  return data;
} catch (error) {
  // Log for debugging
  await writeDebugFile(`failed-analysis-${sessionId}.json`, {
    response: response.text,
    error: error.message,
    timestamp: new Date().toISOString(),
  });

  throw error;
}
```

### Consistency Through Rubrics

We enforce consistent scoring through explicit rubrics:

```
Score Rubric (0-100):
- 90-100: Flawless - user achieved all goals effortlessly
- 70-89: Successful - main goals achieved with minor friction
- 50-69: Mixed - partial success with noticeable challenges
- 30-49: Struggling - significant obstacles prevented goals
- 0-29: Failed - unable to accomplish goals, major issues
```

This rubric ensures different sessions are scored comparably, enabling meaningful trend analysis over time.

## Common Challenges and Solutions

Building a production session analysis system revealed several non-obvious challenges:

### Challenge 1: Session Replay Artifacts

**Problem**: Rrweb recordings often have visual artifacts - missing iframes, broken animations, masked inputs.

**Solution**: Train the AI to recognize and ignore artifacts:

```typescript
const SYSTEM_PROMPT = `
Technical limitations of session replays:
- Iframes may not render (e.g., embedded videos)
- Some animations may be missing or jumpy
- Password fields and sensitive inputs are masked
- System dialogs (print, file upload) won't show
- Window resizing may cause rendering glitches

Focus on user behavior patterns rather than visual perfection.
`;
```

### Challenge 2: Video Generation Timeouts

**Problem**: Long sessions (30+ minutes) can timeout during video generation, especially with limited compute resources.

**Solution**: Use autoscaling Cloud Run instances with webhook callbacks for asynchronous processing:

```typescript
// Main workflow initiates video processing
async function processReplay(sessionId: string) {
  // Create webhook to receive completion callback
  const webhook = await createWebhook();

  // Fire-and-forget request to Cloud Run service
  await fetch("https://cloud-service.run.app/process", {
    method: "POST",
    body: JSON.stringify({
      session_id: sessionId,
      callback: webhook.url, // Where to POST results
    }),
  });

  // Wait for callback (can take 15+ minutes for long sessions)
  const result = await webhook.wait({ timeout: 900000 }); // 15 min

  return result;
}

// Cloud Run service processes synchronously
app.post("/process", async (req, res) => {
  const { session_id, callback } = req.body;

  try {
    // Process recording (long-running, keeps HTTP connection open)
    const video = await recordReplayVideo(session_id);

    // Send results to callback webhook
    await fetch(callback, {
      method: "POST",
      body: JSON.stringify({
        success: true,
        video_uri: video.uri,
        duration: video.duration,
      }),
    });

    res.status(200).json({ success: true });
  } catch (error) {
    res.status(500).json({ success: false, error: error.message });
  }
});
```

Key architectural benefits:

- **Request-based autoscaling**: Each session gets its own Cloud Run instance
- **Long timeout support**: Configure Cloud Run with 15+ minute request timeout
- **No chunking needed**: Each request processes one complete session
- **Automatic scaling**: Handles concurrent sessions without manual orchestration

### Challenge 3: Inconsistent Issue Categorization

**Problem**: The same issue might be classified differently across sessions.

**Solution**: Implement vector similarity matching with human review:

```typescript
// Find similar previously-categorized issues
const similarIssues = await findSimilarIssues(issue.embedding);

if (similarIssues.length > 0) {
  // Use consensus category from similar issues
  const categories = similarIssues.map((i) => i.category);
  const consensus = mostFrequent(categories);

  if (getConfidence(categories) > 0.8) {
    issue.category = consensus;
  } else {
    // Flag for human review
    issue.needs_review = true;
  }
}
```

## Conclusion

Automated AI session replay analysis transforms an overwhelming data problem into a strategic advantage. By combining video understanding with event analysis, we've built a system that analyzes every single user session, surfacing insights that would otherwise remain hidden.

The key architectural decisions that made this possible:

1. **Multi-modal AI input** combining video with structured events
2. **Webhook-based async processing** with autoscaling Cloud Run instances
3. **Structured output schemas** ensuring consistent, actionable data
4. **Native video understanding** using Gemini 2.5 Pro with extended thinking
5. **Production-ready error handling** for replay artifacts and edge cases

The impact extends beyond issue detection. Teams now understand user behavior patterns, identify feature adoption barriers, and catch critical bugs before users report them. Most importantly, the system scales linearly - whether you have 100 or 100,000 sessions, every single one gets analyzed.

The future of session replay isn't watching videos - it's having AI watch them all and tell you what matters.

**Related articles:**

- [How to Use Google Gemini 2.5 Pro for Session Replay Analysis](/blog/gemini-session-replay-analysis)
- [Prompt Engineering for Session Replay Analysis](/blog/prompt-engineering-session-replay)
- [Converting PostHog Recordings to Analyzable Video](/blog/posthog-to-video-pipeline)
